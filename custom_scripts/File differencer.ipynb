{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7e1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import ndiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41b42604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- # --  Purpose:\t\tDag to clean, deps, seed am d run dbt\n",
      "\n",
      "+ # --  Purpose:\t\tDag to clean, deps, seed am d run dbt incrementally\n",
      "\n",
      "?               \t\t                                     ++++++++++++++\n",
      "\n",
      "  # --  Modifications:\n",
      "\n",
      "  # --  WHEN\t\t\t   WHO\t\t\t\t    WHAT\n",
      "\n",
      "- # --  10/08/2023   \tBina Prajapati          PB2-1083 Data Ingestion - DAG to carry Core data through to consume for Rapidrating\n",
      "\n",
      "+ # --  10/12/2023   \tBina Prajapati          Created new Dag to run models incrementally (V2 runs full refresh)\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  import json\n",
      "\n",
      "  import pendulum\n",
      "\n",
      "  import logging as log\n",
      "\n",
      "  import pprint\n",
      "\n",
      "  from airflow import AirflowException\n",
      "\n",
      "  from airflow.decorators import dag, task\n",
      "\n",
      "  from airflow.utils.session import provide_session\n",
      "\n",
      "  from airflow.models import XCom\n",
      "\n",
      "  from airflow.hooks.base import BaseHook\n",
      "\n",
      "  from airflow.models import Variable\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "- DAG_NAME = 'Airflow_Controller_AU_V2'\n",
      "\n",
      "?                                    ^\n",
      "\n",
      "+ DAG_NAME = 'Airflow_Controller_AU_V3_Incremental'\n",
      "\n",
      "?                                    ^^^^^^^^^^^^^\n",
      "\n",
      "  BIGQUERY_CONNECTION = 'PANOROMIQ_BIGQUERY_CONNECTION'\n",
      "\n",
      "  DATAPROC_CONNECTION = 'PANOROMIQ_BIGQUERY_CONNECTION' #'PANORAMIQ_DATAPROC_CONNECTION'\n",
      "\n",
      "  GCS_CONNECTION = 'PANOROMIQ_BIGQUERY_CONNECTION' #'PANORAMIQ_GCS_CONNECTION'\n",
      "\n",
      "  BQ_LOADER_CONFIG_VARIABLE = \"sdpi_bqloader_config_au.json\"\n",
      "\n",
      "  #CONFIG_FILE = 'sdpi_panaromiq_config_au.json'\n",
      "\n",
      "  REGION = \"australia-southeast1\"\n",
      "\n",
      "  infra_env = Variable.get(\"environment\")\n",
      "\n",
      "  run_env = Variable.get(\"run_env\")\n",
      "\n",
      "  geo_env = Variable.get(\"geo_env\")\n",
      "\n",
      "  ee_update_file = '/opt/airflow/dags/schemas/ee_attributes_status_update.sql'\n",
      "\n",
      "  ee_init_tables = '/opt/airflow/dags/schemas/init_tables.sql'\n",
      "\n",
      "  CONFIG_FILE = f\"sdpi_panaromiq_config_{infra_env}_au.json\"\n",
      "\n",
      "  \n",
      "\n",
      "  with open(f'/opt/airflow/dags/dbt/configs/{CONFIG_FILE}', 'r') as f:\n",
      "\n",
      "      config = json.load(f)\n",
      "\n",
      "  \n",
      "\n",
      "  with open(f'/opt/airflow/dags/dbt/configs/{BQ_LOADER_CONFIG_VARIABLE}', 'r') as f:\n",
      "\n",
      "      bq_loader_config = json.load(f)\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  def convert_datetime(datetime_string):\n",
      "\n",
      "      import pytz\n",
      "\n",
      "  \n",
      "\n",
      "      return datetime_string.astimezone(pytz.UTC).strftime('%Y-%m-%d %H:%M:%S')\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  def get_bq_query_results(sql_stmt):\n",
      "\n",
      "      from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\n",
      "\n",
      "  \n",
      "\n",
      "      #hook = BigQueryHook(gcp_conn_id=BIGQUERY_CONNECTION, location='US', use_legacy_sql=False)\n",
      "\n",
      "      hook = BigQueryHook(gcp_conn_id=BIGQUERY_CONNECTION, use_legacy_sql=False)\n",
      "\n",
      "      configuration = {\n",
      "\n",
      "          \"query\": {\n",
      "\n",
      "              \"query\": sql_stmt,\n",
      "\n",
      "              \"useLegacySql\": \"False\"\n",
      "\n",
      "          }\n",
      "\n",
      "      }\n",
      "\n",
      "      job = hook.insert_job(configuration=configuration, nowait=False)\n",
      "\n",
      "      return job.result()\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  def รยง(context):\n",
      "\n",
      "      stage_name = context.get('task_instance').task_id\n",
      "\n",
      "      task = None\n",
      "\n",
      "      if 'job_type' in config[stage_name].keys():\n",
      "\n",
      "          if config[stage_name]['job_type'] == 'dbt':\n",
      "\n",
      "              task = config[stage_name]['dbt_project']\n",
      "\n",
      "      stage = config[stage_name]['stage']\n",
      "\n",
      "      stage_num = config[stage_name]['stage_no']\n",
      "\n",
      "  \n",
      "\n",
      "      status = context.get('task_instance').state\n",
      "\n",
      "      start_date = convert_datetime(context.get('task_instance').start_date)\n",
      "\n",
      "      end_date = convert_datetime(context.get('task_instance').end_date) \\\n",
      "\n",
      "          if context.get('task_instance').end_date is not None else None\n",
      "\n",
      "      duration_sec = (context.get('task_instance').end_date - context.get('task_instance').start_date).total_seconds() \\\n",
      "\n",
      "          if start_date and end_date else 0\n",
      "\n",
      "      run_id = context.get('task_instance').run_id\n",
      "\n",
      "      dataset = config[\"session_manager\"][\"session_dataset\"]\n",
      "\n",
      "      table = config[\"session_manager\"][\"session_table\"]\n",
      "\n",
      "  \n",
      "\n",
      "      ti = context[\"ti\"]\n",
      "\n",
      "  \n",
      "\n",
      "      session_id = ti.xcom_pull(task_ids='generate_session_id')\n",
      "\n",
      "      query = (\n",
      "\n",
      "          f\"SELECT 1 FROM {dataset}.{table} WHERE session_id = {session_id} AND run_id = '{run_id}' \\n\"\n",
      "\n",
      "          f\"AND stage = '{stage}' AND task = '{task}'\"\n",
      "\n",
      "      )\n",
      "\n",
      "      records = get_bq_query_results(query)\n",
      "\n",
      "      is_exist = len(list(records)) >= 1\n",
      "\n",
      "      if is_exist:\n",
      "\n",
      "          query = (\n",
      "\n",
      "              f\"UPDATE {dataset}.{table} SET stage = '{stage}', status= '{status}',\\n\"\n",
      "\n",
      "              f\"start_date ='{start_date}', end_date = '{end_date}', duration_sec = {duration_sec} \\n\"\n",
      "\n",
      "              f\"WHERE session_id = {session_id} AND run_id = '{run_id}' AND  stage = '{stage}' AND task ='{task}' \"\n",
      "\n",
      "          )\n",
      "\n",
      "  \n",
      "\n",
      "      else:\n",
      "\n",
      "          query = (\n",
      "\n",
      "              f\"INSERT {dataset}.{table} \\n\"\n",
      "\n",
      "              f\"(session_id, run_id, stage_num, stage, task, status, start_date, end_date, duration_sec) VALUES \\n\"\n",
      "\n",
      "              f\"({session_id}, '{run_id}', {stage_num}, '{stage}', '{task}', '{status}', '{start_date}', '{end_date}', {duration_sec})\"\n",
      "\n",
      "          )\n",
      "\n",
      "      # pprint.pprint(query)\n",
      "\n",
      "      query_job = get_bq_query_results(query)\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  def skip_if_specified(context):\n",
      "\n",
      "      from airflow.utils.state import State\n",
      "\n",
      "  \n",
      "\n",
      "      task_id = context['task'].task_id\n",
      "\n",
      "      conf = context['dag_run'].conf or {}\n",
      "\n",
      "      skip_tasks = conf.get('skip_tasks', [])\n",
      "\n",
      "      if task_id in skip_tasks:\n",
      "\n",
      "          ti = context['dag_run'].get_task_instance(task_id)\n",
      "\n",
      "          ti.set_state(State.SKIPPED)\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  # Get Max Statements Dataset Version Key\n",
      "\n",
      "  def stmt_get_max(bq_dataset, dvk_dataset, dvk_datasetgroup,\n",
      "\n",
      "                   dvk_datasetsourcexref, dvk_datasetversion, subgroups):\n",
      "\n",
      "      subgroups = ', '.join(['\"{}\"'.format(value) for value in subgroups])\n",
      "\n",
      "      stmt = (f\"\"\"\n",
      "\n",
      "          SELECT b.datasetsubgroup as table_name,\n",
      "\n",
      "              max(d.datasetversionkey) AS task_id,\n",
      "\n",
      "          FROM {bq_dataset}.{dvk_dataset} AS a\n",
      "\n",
      "          JOIN {bq_dataset}.{dvk_datasetgroup} AS b\n",
      "\n",
      "              ON b.datasetgroupkey = a.datasetgroupkey\n",
      "\n",
      "              AND b.datasetsubgroup  in  ({subgroups})\n",
      "\n",
      "          JOIN {bq_dataset}.{dvk_datasetsourcexref} AS c\n",
      "\n",
      "              ON c.datasetid = a.datasetid\n",
      "\n",
      "          JOIN {bq_dataset}.{dvk_datasetversion} AS d\n",
      "\n",
      "              ON d.datasetkey = a.datasetkey\n",
      "\n",
      "          GROUP BY b.datasetsubgroup\n",
      "\n",
      "          \"\"\")\n",
      "\n",
      "      return stmt\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  # Insert Statements Dataset Version Key\n",
      "\n",
      "  def stmt_insert_new(bq_dataset, dvk_dataset, dvk_datasetgroup,\n",
      "\n",
      "                      dvk_datasetsourcexref, dvk_datasetversion, subgroups):\n",
      "\n",
      "      subgroups = ', '.join(['\"{}\"'.format(value) for value in subgroups])\n",
      "\n",
      "      stmt = (f\"\"\"\n",
      "\n",
      "          INSERT INTO {bq_dataset}.{dvk_datasetversion}\n",
      "\n",
      "          SELECT\n",
      "\n",
      "              mx.max_datasetversionkey + ROW_NUMBER() OVER () AS datasetversionkey,\n",
      "\n",
      "              datasetid AS datasetkey,\n",
      "\n",
      "              datasetversionid + 1 AS datasetversionid,\n",
      "\n",
      "              CONCAT(description,' dated ', CURRENT_TIMESTAMP) AS datasetversiondesc,\n",
      "\n",
      "              'idp_hdfs_user' AS addedbyuser,\n",
      "\n",
      "              CURRENT_TIMESTAMP AS insertdt\n",
      "\n",
      "          FROM\n",
      "\n",
      "          (\n",
      "\n",
      "              SELECT\n",
      "\n",
      "                  a.datasetkey,\n",
      "\n",
      "                  a.datasetgroupkey,\n",
      "\n",
      "                  a.datasetid,\n",
      "\n",
      "                  c.datasetname,\n",
      "\n",
      "                  c.description,\n",
      "\n",
      "                  b.datasetgroupname,\n",
      "\n",
      "                  b.datasetsubgroup,\n",
      "\n",
      "                  COALESCE(MAX(d.datasetversionid), 0) AS datasetversionid\n",
      "\n",
      "              FROM\n",
      "\n",
      "                  {bq_dataset}.{dvk_dataset} AS a,\n",
      "\n",
      "                  {bq_dataset}.{dvk_datasetgroup} AS b,\n",
      "\n",
      "                  {bq_dataset}.{dvk_datasetsourcexref} AS c\n",
      "\n",
      "              LEFT JOIN {bq_dataset}.{dvk_datasetversion} AS d\n",
      "\n",
      "                  ON a.datasetkey = d.datasetkey\n",
      "\n",
      "                  WHERE a.datasetgroupkey = b.datasetgroupkey\n",
      "\n",
      "                  AND a.datasetid = c.datasetid\n",
      "\n",
      "                  AND datasetsubgroup in  ({subgroups})\n",
      "\n",
      "              GROUP BY\n",
      "\n",
      "                  a.datasetkey,\n",
      "\n",
      "                  a.datasetgroupkey,\n",
      "\n",
      "                  a.datasetid,\n",
      "\n",
      "                  c.datasetname,\n",
      "\n",
      "                  c.description,\n",
      "\n",
      "                  b.datasetgroupname,\n",
      "\n",
      "                  b.datasetsubgroup\n",
      "\n",
      "          ) AS dataset_det,\n",
      "\n",
      "          (\n",
      "\n",
      "              SELECT\n",
      "\n",
      "                  COALESCE(MAX(datasetversionkey), 0) AS max_datasetversionkey\n",
      "\n",
      "              FROM {bq_dataset}.{dvk_datasetversion}\n",
      "\n",
      "          ) AS mx\n",
      "\n",
      "          \"\"\")\n",
      "\n",
      "      return stmt\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  @task\n",
      "\n",
      "  def generate_session_id():\n",
      "\n",
      "      dataset = config['session_manager'][\"session_dataset\"]\n",
      "\n",
      "      table = config['session_manager'][\"session_table\"]\n",
      "\n",
      "      query = f\"SELECT MAX(session_id) AS session_id FROM {dataset}.{table}\"\n",
      "\n",
      "      results = get_bq_query_results(query)\n",
      "\n",
      "  \n",
      "\n",
      "      for result in results:\n",
      "\n",
      "          if result.session_id is None:\n",
      "\n",
      "              session_id = 1\n",
      "\n",
      "          else:\n",
      "\n",
      "              session_id = result.session_id + 1\n",
      "\n",
      "      log.info(f\"session id: {session_id}\")\n",
      "\n",
      "      return session_id\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  def zipdir_upload(stage, project_path, object_name, files_upload, bucket):\n",
      "\n",
      "      import os\n",
      "\n",
      "      from zipfile import ZipFile\n",
      "\n",
      "      from airflow.providers.google.cloud.hooks.gcs import GCSHook\n",
      "\n",
      "  \n",
      "\n",
      "      log.info('Zipping and transferring files to GCS')\n",
      "\n",
      "  \n",
      "\n",
      "      temp_folder = '/tmp/' + stage\n",
      "\n",
      "      if not os.path.exists(temp_folder):\n",
      "\n",
      "          os.makedirs(temp_folder)\n",
      "\n",
      "  \n",
      "\n",
      "      zippedfile = temp_folder + '/' + 'src.zip'\n",
      "\n",
      "  \n",
      "\n",
      "      # create a ZipFile object\n",
      "\n",
      "      with ZipFile(zippedfile, 'w', allowZip64=True) as zipObj:\n",
      "\n",
      "          # Iterate over all the files in directory\n",
      "\n",
      "          for folderName, subfolders, filenames in os.walk(project_path):\n",
      "\n",
      "              skip = [\"archive\"]\n",
      "\n",
      "              if subfolders in skip:\n",
      "\n",
      "                  continue\n",
      "\n",
      "              else:\n",
      "\n",
      "                  for filename in filenames:\n",
      "\n",
      "                      # create complete filepath of file in directory\n",
      "\n",
      "                      filePath = os.path.join(folderName, filename)\n",
      "\n",
      "                      archive_file_path = os.path.relpath(filePath, project_path)\n",
      "\n",
      "  \n",
      "\n",
      "                      # Add file to zip\n",
      "\n",
      "                      zipObj.write(filePath, archive_file_path)\n",
      "\n",
      "      try:\n",
      "\n",
      "          # upload to GCS\n",
      "\n",
      "          hook = GCSHook(gcp_conn_id=GCS_CONNECTION)\n",
      "\n",
      "          hook.upload(bucket_name=bucket, object_name=object_name + '/' + 'src.zip',\n",
      "\n",
      "                      filename=zippedfile,  timeout=800, num_max_attempts=2)\n",
      "\n",
      "          for filename in files_upload:\n",
      "\n",
      "              extension = filename.split(\".\")[-1]\n",
      "\n",
      "              if extension == 'py':\n",
      "\n",
      "                  mimetype = 'text/x-python'\n",
      "\n",
      "              elif extension == 'sh':\n",
      "\n",
      "                  mimetype = 'text/x-sh'\n",
      "\n",
      "              elif extension == 'jar':\n",
      "\n",
      "                  mimetype = 'application/java-archive'\n",
      "\n",
      "              hook.upload(bucket_name=bucket, object_name=object_name + '/' + filename,\n",
      "\n",
      "                          filename=project_path + '/' + filename, mime_type=mimetype, timeout=800, num_max_attempts=2)\n",
      "\n",
      "      except Exception as ex:\n",
      "\n",
      "          log.error(\"GCS CODE TRANSFER ERROR - {ex}\")\n",
      "\n",
      "          raise AirflowException(\"GCS CODE TRANSFER ERROR - {ex}\")\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  def run_workflow_ephemeral_template(template_name, dataproc_project ):\n",
      "\n",
      "      from airflow.providers.google.cloud.hooks.dataproc import DataprocHook\n",
      "\n",
      "      import google.protobuf.empty_pb2\n",
      "\n",
      "      import time\n",
      "\n",
      "      import traceback\n",
      "\n",
      "  \n",
      "\n",
      "      try:\n",
      "\n",
      "          hook = DataprocHook(gcp_conn_id=DATAPROC_CONNECTION)\n",
      "\n",
      "          operation = hook.instantiate_workflow_template(template_name=template_name, region=REGION,\n",
      "\n",
      "                                                         project_id=dataproc_project)\n",
      "\n",
      "  \n",
      "\n",
      "          loop = True\n",
      "\n",
      "          while loop:\n",
      "\n",
      "              job_done = operation.done()\n",
      "\n",
      "              log.info(operation.metadata)\n",
      "\n",
      "              if job_done:\n",
      "\n",
      "                  loop = False\n",
      "\n",
      "              else:\n",
      "\n",
      "                  time.sleep(180)\n",
      "\n",
      "                  continue\n",
      "\n",
      "          if type(operation.result()) != google.protobuf.empty_pb2.Empty:\n",
      "\n",
      "              raise AirflowException('job result is not an emtpy google protobuf')\n",
      "\n",
      "          else:\n",
      "\n",
      "              log.info('workflow template completed successfully')\n",
      "\n",
      "      except Exception as ex:\n",
      "\n",
      "          # log.error(traceback.print_exc())\n",
      "\n",
      "          raise AirflowException(traceback.print_exc())\n",
      "\n",
      "  \n",
      "\n",
      "  #sdp_code\n",
      "\n",
      "  #def run_dbt(task_config, session_id, table_snaps, dvkeys, target):\n",
      "\n",
      "  \n",
      "\n",
      "  def run_dbt(task_config, target):\n",
      "\n",
      "      import os\n",
      "\n",
      "      import subprocess\n",
      "\n",
      "      import shlex\n",
      "\n",
      "  \n",
      "\n",
      "      #dbt_vars = {'session_id': session_id}\n",
      "\n",
      "      #dbt_vars.update(table_snaps)\n",
      "\n",
      "      #dbt_vars.update(dvkeys)\n",
      "\n",
      "  \n",
      "\n",
      "      conn = BaseHook.get_connection(BIGQUERY_CONNECTION)\n",
      "\n",
      "      with open('/tmp/data.json', 'w') as f:\n",
      "\n",
      "          json.dump(json.loads(conn.extra_dejson[\"extra__google_cloud_platform__keyfile_dict\"]), f)\n",
      "\n",
      "          os.environ['DBT_KEYFILE'] = '/tmp/data.json'\n",
      "\n",
      "      os.environ['CLGX_ENVIRONMENT'] = target\n",
      "\n",
      "      os.environ['INFRA_ENV'] = infra_env\n",
      "\n",
      "      os.environ['RUN_ENV'] = run_env\n",
      "\n",
      "      os.environ['GEO_ENV'] = geo_env\n",
      "\n",
      "  \n",
      "\n",
      "      def dbt_cmd(run_type, target, task_config, tag=None, dbt_vars=None):\n",
      "\n",
      "          profiles_dir = task_config['profiles_dir']\n",
      "\n",
      "          project_dir = task_config['dbt_project']\n",
      "\n",
      "          log_format = task_config['log_format']\n",
      "\n",
      "          refresh = task_config['full_refresh']\n",
      "\n",
      "  \n",
      "\n",
      "          cmd = ['dbt']\n",
      "\n",
      "          if log_format == 'json':\n",
      "\n",
      "              cmd.append('--log-format json')\n",
      "\n",
      "          cmd.append(run_type)\n",
      "\n",
      "          if target is not None:\n",
      "\n",
      "              cmd.append(f\"--target {target}\")\n",
      "\n",
      "          if project_dir is not None:\n",
      "\n",
      "              cmd.append(f\"--project-dir /opt/airflow/dags/dbt/{project_dir}\")\n",
      "\n",
      "              if profiles_dir is not None:\n",
      "\n",
      "                  cmd.append(f\"--profiles-dir /opt/airflow/dags/dbt/{project_dir}/{profiles_dir}\")\n",
      "\n",
      "          if run_type == 'run':\n",
      "\n",
      "              if tag is not None:\n",
      "\n",
      "                  cmd.append(f\"--models tag:{tag}\")\n",
      "\n",
      "          if run_type == 'run' or run_type == 'test':\n",
      "\n",
      "              if dbt_vars is not None:\n",
      "\n",
      "                  cmd.append(f\"--vars '{dbt_vars}'\")\n",
      "\n",
      "-             if refresh == 'true':\n",
      "\n",
      "-                 cmd.append('--full-refresh')\n",
      "\n",
      "  \n",
      "\n",
      "          print(f\"command built {cmd}\")\n",
      "\n",
      "          return ' '.join(cmd)\n",
      "\n",
      "  \n",
      "\n",
      "      bash_command = [dbt_cmd('clean', target, task_config),\n",
      "\n",
      "                      dbt_cmd('deps', target, task_config),\n",
      "\n",
      "                      dbt_cmd('seed', target, task_config, None),\n",
      "\n",
      "                      dbt_cmd('run', target, task_config, None)\n",
      "\n",
      "                      #,\n",
      "\n",
      "                      #dbt_cmd('run', target, task_config, 'pub', dbt_vars),\n",
      "\n",
      "                      #dbt_cmd('test', target, task_config, None)\n",
      "\n",
      "                      ]\n",
      "\n",
      "  \n",
      "\n",
      "      for cmd in bash_command:\n",
      "\n",
      "          sp = subprocess.Popen(shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True)\n",
      "\n",
      "          log.info(\"Output:\")\n",
      "\n",
      "          for line in iter(sp.stdout.readline, b''):\n",
      "\n",
      "              line = line.decode('utf-8').rstrip()\n",
      "\n",
      "              log.info(line)\n",
      "\n",
      "          sp.wait()\n",
      "\n",
      "          log.info(\"Command exited with return code %s\", sp.returncode)\n",
      "\n",
      "  \n",
      "\n",
      "          if sp.returncode:\n",
      "\n",
      "              raise AirflowException(\"dbt command failed\")\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  def run_pyspark(dataproc_config, bucket, dataproc_project, environment):\n",
      "\n",
      "      zipdir_upload(dataproc_config['stage'], '/opt/airflow/dbt/dataproc/' + dataproc_config['pyspark_project'],\n",
      "\n",
      "                    dataproc_config['project'] + '-' + dataproc_config['usecase'], dataproc_config['gcs_upload'],\n",
      "\n",
      "                    bucket)\n",
      "\n",
      "      run_workflow_ephemeral_template(dataproc_config['project'] + '-' + environment + '-' + dataproc_config['usecase'],\n",
      "\n",
      "                                      dataproc_project)\n",
      "\n",
      "  \n",
      "\n",
      "  def read_file(file):\n",
      "\n",
      "      with open(file, 'r') as f:\n",
      "\n",
      "          return f.read()\n",
      "\n",
      "  \n",
      "\n",
      "  @task\n",
      "\n",
      "  # Understand what is Data key version keys generator does\n",
      "\n",
      "  def generate_dataversion_keys():\n",
      "\n",
      "      bq_dataset = config['dvkey_generator'][\"bq_dataset\"]\n",
      "\n",
      "      dvk_dataset = config['dvkey_generator'][\"dvk_dataset\"]\n",
      "\n",
      "      dvk_datasetgroup = config['dvkey_generator'][\"dvk_datasetgroup\"]\n",
      "\n",
      "      dvk_datasetsourcexref = config['dvkey_generator'][\"dvk_datasetsourcexref\"]\n",
      "\n",
      "      dvk_datasetversion = config['dvkey_generator'][\"dvk_datasetversion\"]\n",
      "\n",
      "      subgroups = config['dvkey_generator'][\"subgroups\"]\n",
      "\n",
      "  \n",
      "\n",
      "      query = stmt_insert_new(\n",
      "\n",
      "          bq_dataset,\n",
      "\n",
      "          dvk_dataset,\n",
      "\n",
      "          dvk_datasetgroup,\n",
      "\n",
      "          dvk_datasetsourcexref,\n",
      "\n",
      "          dvk_datasetversion,\n",
      "\n",
      "          subgroups\n",
      "\n",
      "      )\n",
      "\n",
      "      results = get_bq_query_results(query)\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  @task\n",
      "\n",
      "  def load_dataversion_keys():\n",
      "\n",
      "      bq_dataset = config['dvkey_generator'][\"bq_dataset\"]\n",
      "\n",
      "      dvk_dataset = config['dvkey_generator'][\"dvk_dataset\"]\n",
      "\n",
      "      dvk_datasetgroup = config['dvkey_generator'][\"dvk_datasetgroup\"]\n",
      "\n",
      "      dvk_datasetsourcexref = config['dvkey_generator'][\"dvk_datasetsourcexref\"]\n",
      "\n",
      "      dvk_datasetversion = config['dvkey_generator'][\"dvk_datasetversion\"]\n",
      "\n",
      "      subgroups = config['dvkey_generator'][\"subgroups\"]\n",
      "\n",
      "  \n",
      "\n",
      "      query = stmt_get_max(bq_dataset,\n",
      "\n",
      "                           dvk_dataset,\n",
      "\n",
      "                           dvk_datasetgroup,\n",
      "\n",
      "                           dvk_datasetsourcexref,\n",
      "\n",
      "                           dvk_datasetversion,\n",
      "\n",
      "                           subgroups)\n",
      "\n",
      "      results = get_bq_query_results(query)\n",
      "\n",
      "      dvkeys_version = {}\n",
      "\n",
      "      for result in results:\n",
      "\n",
      "          if result[1] is not None:\n",
      "\n",
      "              try:\n",
      "\n",
      "                  extract_id = int(result[1])\n",
      "\n",
      "              except:\n",
      "\n",
      "                  extract_id = result[1]\n",
      "\n",
      "              dvkeys_version[result[0]] = extract_id\n",
      "\n",
      "      log.info(\"data set version keys - \")\n",
      "\n",
      "      pprint.pprint(dvkeys_version)\n",
      "\n",
      "      return dvkeys_version\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  @task\n",
      "\n",
      "  def bq_table_snapshots():\n",
      "\n",
      "      table_snaps = {}\n",
      "\n",
      "      sql_cmd = []\n",
      "\n",
      "      for item in bq_loader_config['loader']:\n",
      "\n",
      "          table_names = ', '.join(f\"'{item}'\" for item in item['table'])\n",
      "\n",
      "          sql_cmd.append(\n",
      "\n",
      "              f\"select split(table_name, '_')[SAFE_OFFSET(0)] as table_name, \"\n",
      "\n",
      "              f\"max(split(table_name, '_')[SAFE_OFFSET(1)]) as extract_id \"\n",
      "\n",
      "              f\"from {item['dataset']}.INFORMATION_SCHEMA.TABLES WHERE table_type='BASE TABLE' \"\n",
      "\n",
      "              f\"and (regexp_contains(split(table_name, '_')[SAFE_OFFSET(1)], r'\\d')) \"\n",
      "\n",
      "              f\"and (split(table_name, '_')[SAFE_OFFSET(0)] in ( {table_names} )) group by table_name\"\n",
      "\n",
      "          )\n",
      "\n",
      "      sql = \" UNION DISTINCT \".join(f\"{item}\" for item in sql_cmd)\n",
      "\n",
      "      results = get_bq_query_results(sql)\n",
      "\n",
      "  \n",
      "\n",
      "      for result in results:\n",
      "\n",
      "          if result[1] is not None:\n",
      "\n",
      "              try:\n",
      "\n",
      "                  extract_id = int(result[1])\n",
      "\n",
      "              except:\n",
      "\n",
      "                  extract_id = result[1]\n",
      "\n",
      "              table_snaps[result[0]] = extract_id\n",
      "\n",
      "      log.info(\"bq loader tables and extract ids -\")\n",
      "\n",
      "      pprint.pprint(table_snaps)\n",
      "\n",
      "      return table_snaps\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  @task(retries=0)\n",
      "\n",
      "  @provide_session\n",
      "\n",
      "  def cleanup_xcom(session=None, **context):\n",
      "\n",
      "      dag_run = context[\"dag_run\"]\n",
      "\n",
      "      execution_date = dag_run.execution_date\n",
      "\n",
      "      dag = context[\"dag\"]\n",
      "\n",
      "      dag_id = dag._dag_id\n",
      "\n",
      "  \n",
      "\n",
      "      # It will delete all xcom of the dag_id\n",
      "\n",
      "      session.query(XCom).filter(XCom.dag_id == dag_id).filter(XCom.execution_date == execution_date).delete(\n",
      "\n",
      "          synchronize_session='fetch')\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  default_args = {\n",
      "\n",
      "      'retries': 0,\n",
      "\n",
      "      'pre_execute': skip_if_specified,\n",
      "\n",
      "      'trigger_rule': 'none_failed'\n",
      "\n",
      "  }\n",
      "\n",
      "  \n",
      "\n",
      "  @task()\n",
      "\n",
      "  def update_ee_status():\n",
      "\n",
      "      file_content = read_file(ee_update_file)\n",
      "\n",
      "      records = get_bq_query_results(file_content)\n",
      "\n",
      "  \n",
      "\n",
      "  @task()\n",
      "\n",
      "  def init_tables():\n",
      "\n",
      "      file_content = read_file(ee_init_tables)\n",
      "\n",
      "-     records = get_bq_query_results(file_content)    \n",
      "\n",
      "?                                                 ----\n",
      "\n",
      "+     records = get_bq_query_results(file_content)\n",
      "\n",
      "  \n",
      "\n",
      "  @dag(\n",
      "\n",
      "      dag_id=DAG_NAME,\n",
      "\n",
      "      default_args=default_args,\n",
      "\n",
      "      description='SDPI AU DAG based on PanaromicIQ DAG',\n",
      "\n",
      "      schedule_interval=None,\n",
      "\n",
      "      start_date=pendulum.datetime(2022, 10, 1, tz=\"UTC\"),\n",
      "\n",
      "      catchup=False,\n",
      "\n",
      "      is_paused_upon_creation=False\n",
      "\n",
      "  )\n",
      "\n",
      "  def panoramiq_controller():\n",
      "\n",
      "      #Note : sdp_code\n",
      "\n",
      "      #session_id = generate_session_id()\n",
      "\n",
      "      #table_snapshots = bq_table_snapshots()\n",
      "\n",
      "      #generatedata_version_keys = generate_dataversion_keys()\n",
      "\n",
      "      #load_data_version_keys = load_dataversion_keys()\n",
      "\n",
      "  \n",
      "\n",
      "      # Define tasks\n",
      "\n",
      "      tasks = {}\n",
      "\n",
      "  \n",
      "\n",
      "      tasks[\"init_tables\"] = init_tables()\n",
      "\n",
      "  \n",
      "\n",
      "      tasks[\"update_ee_status\"] = update_ee_status()\n",
      "\n",
      "  \n",
      "\n",
      "      filtered_dict = {k: v for (k, v) in config.items() if 'job_type' in v}\n",
      "\n",
      "      for key, values in filtered_dict.items():\n",
      "\n",
      "          if values['job_type'] == 'dbt':\n",
      "\n",
      "              tasks[\"{}\".format(key)] = task(task_id=f\"{key}\",\n",
      "\n",
      "                                      #sdp_code comment\n",
      "\n",
      "                                      #    on_execute_callback=notifications,\n",
      "\n",
      "                                      #    on_failure_callback=notifications,\n",
      "\n",
      "                                      #    on_success_callback=notifications,\n",
      "\n",
      "                                      #    on_retry_callback=notifications)\n",
      "\n",
      "                  )(run_dbt)(values, config[\"clgx_environment\"])\n",
      "\n",
      "  # Original sdp_code\n",
      "\n",
      "  #               (run_dbt)(values, session_id, table_snapshots, load_data_version_keys, config[\"clgx_environment\"])\n",
      "\n",
      "  \n",
      "\n",
      "          if values['job_type'] == 'pyspark':\n",
      "\n",
      "              tasks[\"{}\".format(key)] = task(task_id=f\"{key}\",\n",
      "\n",
      "                                             # on_execute_callback=notifications,\n",
      "\n",
      "                                             # on_failure_callback=notifications,\n",
      "\n",
      "                                             # on_success_callback=notifications,\n",
      "\n",
      "                                             # on_retry_callback=notifications\n",
      "\n",
      "                                             )(run_pyspark)(values, config[\"gcs_bucket\"], config[\"dataproc_project\"],\n",
      "\n",
      "                                                            config[\"clgx_environment\"])\n",
      "\n",
      "  \n",
      "\n",
      "     # clean_xcom = cleanup_xcom()\n",
      "\n",
      "  \n",
      "\n",
      "      tasks[\"init_tables\"] >> tasks[\"ee\"] >> tasks[\"update_ee_status\"]\n",
      "\n",
      "  \n",
      "\n",
      "     # [session_id, bq_table_snapshots, generatedata_version_keys]\n",
      "\n",
      "     # generatedata_version_keys >> load_data_version_keys >> tasks[\"msp\"] >> tasks[\"standardizer\"]\n",
      "\n",
      "     # tasks[\"standardizer\"] >> [tasks[\"srb_enterprise\"], tasks[\"srb_insurance\"], tasks[\"srb_marketing\"]] >> clean_xcom\n",
      "\n",
      "  \n",
      "\n",
      "  dag = panoramiq_controller()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"Airflow_Controller_AU_v2.py\") as a, open(\"Airflow_Controller_AU_v3_Incremental.py\") as b:\n",
    "    for dif in ndiff(a.readlines(),b.readlines()):\n",
    "        print(dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9323526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
