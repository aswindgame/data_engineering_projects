{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMwBt/fh2jBGAHhLOG9TG9d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gj1OqWdvtI8Y","executionInfo":{"status":"ok","timestamp":1711170561314,"user_tz":-330,"elapsed":83493,"user":{"displayName":"Mugundan Aswin","userId":"03807238789660081623"}},"outputId":"69614e76-a3e6-48f7-a5e0-73621ab5a4f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["The Webpage responded with <Response [200]>\n","Total jobs found is 96\n","Fetching the Job details.....\n","Fetching small JD\n","The Final json is stored in local directory\n"]}],"source":["#!/usr/bin/env python\n","# coding: utf-8\n","\n","# In[124]:\n","\n","\n","#Importing necessary libraries\n","from bs4 import BeautifulSoup as bs\n","import html5lib\n","import requests as rq\n","import pandas as pd\n","import numpy as np\n","import json\n","\n","\n","# In[127]:\n","\n","\n","#Scrapping using BeautifulSoup and converting t\n","url = 'https://www.cermati.com/karir'\n","HEAD = ({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36', 'Accept-Language': 'en-US, en;q=0.5'})\n","webpage = rq.get(url, headers=HEAD)\n","soup = bs(webpage.content, \"html.parser\")\n","print(\"The Webpage responded with\", webpage)\n","\n","\n","# In[4]:\n","\n","\n","#Extracting the required data and converting to dictionary\n","ext = soup.find(\"script\", attrs={'id': \"initials\" }).text\n","jobs_list = json.loads(ext)['smartRecruiterResult']['all']['content']\n","\n","\n","# In[123]:\n","\n","\n","#Extracting the Job ID's\n","job_ids={}\n","for i in jobs_list:\n","    job_ids[i['name']]=i['id']\n","print(\"Total jobs found is\", len(job_ids))\n","\n","\n","# In[ ]:\n","\n","\n","\"\"\"\n","1. We can get the department, location, job_type details from main url page\n","2. we can get the description and qualification from job url page\n","So, We are going to get the data from both page and merge to form a json file\n","\"\"\"\n","\n","\n","# In[81]:\n","\n","\n","# 2.1 Defining a function to fetch job details from each job page\n","def full_jd(o):\n","    url = 'https://www.smartrecruiters.com/Cermaticom/' + o\n","    headers = {\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36',\n","        'Accept-Language': 'en-US, en;q=0.5'\n","    }\n","    webpage = rq.get(url, headers=headers)\n","    soup = bs(webpage.content, 'html.parser')\n","    fin_jd = {}\n","    try:\n","        job_desc = soup.find(\"div\", attrs={'class': 'wysiwyg', 'itemprop':'responsibilities'}).find(\"ul\").find_all('li')\n","        a1 = []\n","        for i in job_desc:\n","            a1.append(i.text)\n","        job_qual = soup.find(\"div\", attrs={'class': 'wysiwyg', 'itemprop':'qualifications'}).find(\"ul\").find_all('li')\n","        a2 = []\n","        for i in job_qual:\n","            a2.append(i.text)\n","            fin_jd = {'Description': a1, 'Qualification': a2}\n","        return fin_jd\n","    except:\n","        pass\n","\n","\n","# In[82]:\n","\n","\n","# 2.2 Forming a combined description and qualification jd of all jobs\n","print(\"Fetching the Job details.....\")\n","big_jd = {}\n","for i, j in job_ids.items():\n","    big_jd[i] = full_jd(j)\n","\n","\n","# In[94]:\n","\n","\n","# 2.3 Defining a function to remove NOne values\n","def remove_none_values(dictionary):\n","    new_dictionary = {}\n","    try:\n","        for key, value in dictionary.items():\n","            if value is not None:\n","                new_dictionary[key] = value\n","        return new_dictionary\n","    except:\n","        pass\n","big_jd = remove_none_values(big_jd)\n","\n","\n","# In[69]:\n","\n","\n","# 1.1 Defining functions to fetch data from main url page\n","def find_dep(o):\n","    try:\n","        return o['department']['label']\n","    except:\n","        pass\n","def jd(l):\n","    n = {}\n","    try:\n","        n['title'] = l['name']\n","        n['location'] = l['location']['city'] + \", \" + l['customField'][2]['valueLabel']\n","        n['release_date'] = l['releasedDate']\n","        n['job_type'] = l['typeOfEmployment']['id'] + \", \" + l['typeOfEmployment']['label']\n","        return n\n","    except:\n","        pass\n","\n","\n","# In[20]:\n","\n","\n","# 1.2 Creating a for loop to iterate over each job in URL main page\n","print(\"Fetching small JD\")\n","small_jd = {}\n","for s in jobs_list:\n","    if find_dep(s) in small_jd.keys():\n","        small_jd[find_dep(s)].append(jd(s))\n","    else:\n","        lister = []\n","        lister.append(jd(s))\n","        small_jd[find_dep(s)] = lister\n","\n","\n","# In[22]:\n","\n","\n","# 1.3 Removing None items\n","small_jd.pop(None)\n","\n","\n","# In[101]:\n","\n","\n","# Merging output small_jd and big_jd\n","def merging_jd(small_jd, big_jd):\n","    for i, j in small_jd.items():\n","        for o in small_jd[i]:\n","            for l, m in big_jd.items():\n","                if o['title'] == l:\n","                    o.update(big_jd[l])\n","    return small_jd\n","\n","\n","# In[102]:\n","\n","\n","# Merging function\n","solution = merging_jd(small_jd, big_jd)\n","\n","\n","# In[106]:\n","\n","\n","# Converting to json\n","solution_json = json.dumps(solution, indent=4)\n","\n","\n","# In[ ]:\n","\n","\n","with open('solution.json', mode='wt', encoding='utf-8') as saver:\n","    saver.writelines(solution_json)\n","print(\"The Final json is stored in local directory\")"]}]}